{
    "abstract": "We analyze the performance of a class of manifold-learning\nalgorithms that find their output by minimizing a quadratic form\nunder some normalization constraints. This class consists of\nLocally Linear Embedding (LLE), Laplacian Eigenmap, Local Tangent\nSpace Alignment (LTSA), Hessian Eigenmaps (HLLE), and Diffusion\nmaps. We present and prove conditions on the manifold that are\nnecessary for the success of the algorithms. Both the finite\nsample case and the limit case are analyzed. We show that there\nare simple manifolds in which the necessary conditions are\nviolated, and hence the algorithms cannot recover the underlying\nmanifolds. Finally, we present numerical results that demonstrate\nour claims.",
    "authors": [
        "Yair Goldberg",
        "Alon Zakai",
        "Dan Kushnir",
        "Ya'acov Ritov"
    ],
    "id": "goldberg08a",
    "issue": 62,
    "pages": [
        1909,
        1939
    ],
    "title": "Manifold Learning: The Price of Normalization",
    "volume": "9",
    "year": "2008"
}